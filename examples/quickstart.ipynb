{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1871a0b-b84b-4075-8358-5a6c49749f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -e .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "365c73bb-9656-420a-8136-0e083c792b7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-10 12:40:39,833\tWARNING services.py:2070 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 407871488 bytes available. This will harm performance! You may be able to free up space by deleting files in /dev/shm. If you are inside a Docker container, you can increase /dev/shm size by passing '--shm-size=0.86gb' to 'docker run' (or add it to the run_options list in a Ray cluster config). Make sure to set this to more than 30% of available RAM.\n",
      "2025-11-10 12:40:41,000\tINFO worker.py:1852 -- Started a local Ray instance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered + ranking:\n",
      "  grupo_quinquenal_de_edad              islas   sexo               periodo  \\\n",
      "0         Todas las edades  07 Balears, Illes  Total    1 de julio de 2002   \n",
      "1         Todas las edades  07 Balears, Illes  Total  1 de octubre de 2002   \n",
      "2         Todas las edades  07 Balears, Illes  Total    1 de enero de 2003   \n",
      "3         Todas las edades  07 Balears, Illes  Total    1 de enero de 2002   \n",
      "4         Todas las edades  07 Balears, Illes  Total    1 de abril de 2002   \n",
      "\n",
      "     total  total_amount rank_by_amount  \n",
      "0  866.087          1000              1  \n",
      "1       ..          1000              2  \n",
      "2  883.410          1000              3  \n",
      "3  845.130          1000              4  \n",
      "4       ..          1000              5  \n",
      "Row count after filter:\n",
      "   row_count\n",
      "0      67830\n",
      "✔️ Eliminada tabla temporal: temp_db.temp_def85a12\n",
      "✅ Sesión finalizada y temporales eliminados.\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# Quickstart: Athena Bridge + Spark (+ optional AWS Wrangler)\n",
    "# ================================\n",
    "\n",
    "python = True  # True to run with athena_bridge (Python), False to run with PySpark (EMR cluster or Glue Interactive Session)\n",
    "\n",
    "# --- Main imports (Spark-like API exposed by athena_bridge)\n",
    "if python:\n",
    "    import athena_bridge.functions as F\n",
    "    import athena_bridge.data_types as T\n",
    "    from athena_bridge.window import Window as W\n",
    "    Window = W()\n",
    "    from athena_bridge.spark_athena_bridge import get_spark\n",
    "else:\n",
    "    import pyspark.sql.functions as F\n",
    "    import pyspark.sql.types as T\n",
    "    from pyspark.sql.window import Window\n",
    "    from pyspark.sql import SparkSession\n",
    "\n",
    "# --- Glue/Athena/S3 integration\n",
    "import awswrangler as wr\n",
    "\n",
    "# ================================\n",
    "# 1) Configuration parameters\n",
    "# ================================\n",
    "BASE_DATOS_TMP     = \"__YOUR_ATHENA_DATABASE_TEMP_FOR_ATHENA_BRIDGE__\"   # Temporary database in Glue/Athena\n",
    "DIRECTORIO_TMP_S3  = \"s3://__YOUR_S3_PATH_TEMP_FOR_ATHENA_BRIDGE__\"  # Staging path\n",
    "WORKGROUP          = \"__YOUR_ATHENA_WORKGROUP__\"\n",
    "\n",
    "\n",
    "# Example paths (adjust as needed)\n",
    "\n",
    "# In Athena, CSV files are read through the directory that contains them — \n",
    "# you cannot specify the full path including the CSV file name.\n",
    "# The directory must contain only CSV files with the same structure.\n",
    "# If you want to read a single CSV file, its parent directory should contain only that file.\n",
    "S3_IN_CSV          = \"s3://__YOUR_S3_PATH_TO_READ_CSV_/\"\n",
    "S3_IN_JSONL        = \"s3://__YOUR_S3_PATH_TO_READ_JSON_/\"   # JSON Lines (optional)\n",
    "S3_IN_PARQUET      = \"s3://__YOUR_S3_PATH_TO_READ_PARQUET_/\" # Existing Parquet (optional)\n",
    "\n",
    "S3_OUT_PARQUET     = \"s3://__YOUR_S3_PATH_TO_SAVE_PARQUET__/\"\n",
    "GLUE_TABLE         = \"example_table\"  # Table name in Glue/Athena\n",
    "\n",
    "# ================================\n",
    "# 2) Spark session\n",
    "# ================================\n",
    "# - Uses the selected workgroup (ideally with \"Enforce workgroup configuration\" enabled)\n",
    "# - DIRECTORIO_TMP_S3 is used for temporary staging\n",
    "if python:\n",
    "    spark = get_spark(\n",
    "        database_tmp=BASE_DATOS_TMP,\n",
    "        path_tmp=DIRECTORIO_TMP_S3,\n",
    "        workgroup=WORKGROUP\n",
    "    )\n",
    "else:\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# ================================\n",
    "# 3) Read common formats\n",
    "# ================================\n",
    "\n",
    "# 3a) CSV (with header and ';' as separator)\n",
    "df_csv = (\n",
    "    spark.read\n",
    "         .format(\"csv\")\n",
    "         .option(\"header\", True)\n",
    "         .option(\"sep\", \";\")\n",
    "         .load(S3_IN_CSV)\n",
    ")\n",
    "\n",
    "# 3b) JSON Lines (one JSON object per line). Omit if you don't have JSONL.\n",
    "# df_jsonl = (\n",
    "#     spark.read\n",
    "#          .format(\"json\")\n",
    "#          .option(\"multiLine\", False)   # JSON Lines\n",
    "#          .load(S3_IN_JSONL)\n",
    "# )\n",
    "\n",
    "# 3c) Existing Parquet (if you have another Parquet folder)\n",
    "# df_parquet_in = spark.read.format(\"parquet\").load(S3_IN_PARQUET)\n",
    "\n",
    "# ================================\n",
    "# 4) Write as Parquet (snappy by default)\n",
    "# ================================\n",
    "# Save the CSV we just read as a clean Parquet dataset\n",
    "(df_csv\n",
    " .write\n",
    " .format(\"parquet\")\n",
    " .mode(\"overwrite\")              # caution: overwrites the target prefix\n",
    " .save(S3_OUT_PARQUET)\n",
    ")\n",
    "\n",
    "# ================================\n",
    "# 5) Publish metadata to Glue (so Athena can query it)\n",
    "# ================================\n",
    "# Important: point to a prefix that contains ONLY .parquet\n",
    "wr.s3.store_parquet_metadata(\n",
    "    path=S3_OUT_PARQUET,\n",
    "    database=BASE_DATOS_TMP,\n",
    "    table=GLUE_TABLE,\n",
    "    dataset=True,                     # treat the prefix as a dataset (with or without Hive-style partitions)\n",
    "    mode=\"overwrite\",                 # create/update the table\n",
    ")\n",
    "\n",
    "# ================================\n",
    "# 6) Read the table from Glue/Athena\n",
    "# ================================\n",
    "df_table = spark.read.table(f\"{BASE_DATOS_TMP}.{GLUE_TABLE}\")\n",
    "\n",
    "# ================================\n",
    "# 7) DataFrame operations\n",
    "# ================================\n",
    "\n",
    "# 7a) Create a constant column\n",
    "df_table = df_table.withColumn(\"total_amount\", F.lit(1000))\n",
    "\n",
    "# 7b) Simple filter\n",
    "df_filtered = df_table.filter(F.col(\"total_amount\") >= 1000)\n",
    "\n",
    "# 7c) Example aggregation (row count)\n",
    "df_agg = df_filtered.agg(F.count(\"total_amount\").alias(\"row_count\"))\n",
    "\n",
    "# 7d) Window: rank by total_amount (didactic example)\n",
    "#     Define a window ordered by the column we just added.\n",
    "win = Window.orderBy(F.col(\"total_amount\").desc())\n",
    "df_rank = df_filtered.withColumn(\"rank_by_amount\", F.row_number().over(win))\n",
    "\n",
    "# (Optional) Show a few rows\n",
    "print(\"Filtered + ranking:\")\n",
    "print(df_rank.head())         # returns a list of Row\n",
    "print(\"Row count after filter:\")\n",
    "print(df_agg.head())\n",
    "\n",
    "# ================================\n",
    "# 8) Clean shutdown\n",
    "# ================================\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea1af1e-aa82-43f7-8c5f-9534aa43ca05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
