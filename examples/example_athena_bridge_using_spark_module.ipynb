{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining file:///home/sagemaker-user/athena_bridge\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Checking if build backend supports build_editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing editable metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: awswrangler>=3.10.0 in /opt/conda/lib/python3.12/site-packages (from athena_bridge==0.0.2) (3.14.0)\n",
      "Requirement already satisfied: boto3<2,>=1.20.32 in /opt/conda/lib/python3.12/site-packages (from awswrangler>=3.10.0->athena_bridge==0.0.2) (1.37.3)\n",
      "Requirement already satisfied: botocore<2,>=1.23.32 in /opt/conda/lib/python3.12/site-packages (from awswrangler>=3.10.0->athena_bridge==0.0.2) (1.37.3)\n",
      "Requirement already satisfied: numpy<3.0,>=1.26 in /opt/conda/lib/python3.12/site-packages (from awswrangler>=3.10.0->athena_bridge==0.0.2) (1.26.4)\n",
      "Requirement already satisfied: packaging<26.0,>=21.1 in /opt/conda/lib/python3.12/site-packages (from awswrangler>=3.10.0->athena_bridge==0.0.2) (24.2)\n",
      "Requirement already satisfied: pandas<3.0.0,>=1.2.0 in /opt/conda/lib/python3.12/site-packages (from awswrangler>=3.10.0->athena_bridge==0.0.2) (2.3.3)\n",
      "Requirement already satisfied: pyarrow<22.0.0,>=8.0.0 in /opt/conda/lib/python3.12/site-packages (from awswrangler>=3.10.0->athena_bridge==0.0.2) (19.0.1)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.12/site-packages (from awswrangler>=3.10.0->athena_bridge==0.0.2) (80.9.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /opt/conda/lib/python3.12/site-packages (from awswrangler>=3.10.0->athena_bridge==0.0.2) (4.15.0)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.12/site-packages (from boto3<2,>=1.20.32->awswrangler>=3.10.0->athena_bridge==0.0.2) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.12.0,>=0.11.0 in /opt/conda/lib/python3.12/site-packages (from boto3<2,>=1.20.32->awswrangler>=3.10.0->athena_bridge==0.0.2) (0.11.3)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.12/site-packages (from botocore<2,>=1.23.32->awswrangler>=3.10.0->athena_bridge==0.0.2) (2.9.0.post0)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /opt/conda/lib/python3.12/site-packages (from botocore<2,>=1.23.32->awswrangler>=3.10.0->athena_bridge==0.0.2) (1.26.20)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.12/site-packages (from pandas<3.0.0,>=1.2.0->awswrangler>=3.10.0->athena_bridge==0.0.2) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.12/site-packages (from pandas<3.0.0,>=1.2.0->awswrangler>=3.10.0->athena_bridge==0.0.2) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.12/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<2,>=1.23.32->awswrangler>=3.10.0->athena_bridge==0.0.2) (1.17.0)\n",
      "Building wheels for collected packages: athena_bridge\n",
      "  Building editable for athena_bridge (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for athena_bridge: filename=athena_bridge-0.0.2-0.editable-py3-none-any.whl size=6498 sha256=cb6951e8af1a940d0f9cdb1e005004553a2242c6eb913cc63cc38847376bf6e5\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-8p8vet5o/wheels/32/99/bc/4c7ada3e84e2673f4d4776e044d89dca78028a63859a1ae19e\n",
      "Successfully built athena_bridge\n",
      "Installing collected packages: athena_bridge\n",
      "  Attempting uninstall: athena_bridge\n",
      "    Found existing installation: athena_bridge 0.0.2\n",
      "    Uninstalling athena_bridge-0.0.2:\n",
      "      Successfully uninstalled athena_bridge-0.0.2\n",
      "Successfully installed athena_bridge-0.0.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -e .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hkZomoeNuYZH"
   },
   "source": [
    "# 0. Execution Mode\n",
    "\n",
    "We define the execution mode, which allows this notebook to run either with **PySpark** (on an EMR cluster or Glue Interactive Session — note that in GIS you must add and run the session configuration cells at the beginning of the notebook) or with **Python**.  \n",
    "Set the variable to **True** to run with Python + Athena, or leave it as **False** to run with PySpark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "SKHVXoaguVOv"
   },
   "outputs": [],
   "source": [
    "python = True  # True to run with athena_bridge (Python), False to run with PySpark (EMR cluster or Glue Interactive Session)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yz7V2dJsOmX2"
   },
   "source": [
    "# 1. Library Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "IKFxet-dOovq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: unknown command \"athena_bridge\"\n"
     ]
    }
   ],
   "source": [
    "if python:\n",
    "  !pip athena_bridge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cjpp6vGfOsez"
   },
   "source": [
    "# 2. Library Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "0hnVja8wOrjX"
   },
   "outputs": [],
   "source": [
    "if python:\n",
    "  import athena_bridge.functions as F\n",
    "  import athena_bridge.data_types as T\n",
    "  from athena_bridge.window import Window as W\n",
    "  Window = W()\n",
    "  from athena_bridge.spark_athena_bridge import get_spark\n",
    "else:\n",
    "  import pyspark.sql.functions as F\n",
    "  import pyspark.sql.types as T\n",
    "  from pyspark.sql.window import Window\n",
    "  from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "from datetime import *\n",
    "import pandas as pd\n",
    "import time\n",
    "import functools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JksNnSlaO2Tk"
   },
   "source": [
    "# 3. Dataproc Athena Bridge Creation for Table and File Reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "I1fcWGM2O0Z3"
   },
   "outputs": [],
   "source": [
    "if python:\n",
    "  # The database **datatemp** does not exist by default. As a temporary database, we can use any existing\n",
    "  # database in Athena or create a new one for this purpose. If you want to use **datatemp**, \n",
    "  # you can create it in Athena by running:\n",
    "  # CREATE DATABASE datatemp\n",
    "\n",
    "  base_datos_temporal = \"__YOUR_ATHENA_DATABASE_TEMP_FOR_ATHENA_BRIDGE__\"   # Temporary database in Glue/Athena\n",
    "  directorio_temporal = \"s3://__YOUR_S3_PATH_TEMP_FOR_ATHENA_BRIDGE__\"  # Staging path\n",
    "  workgroup = '__YOUR_ATHENA_WORKGROUP__'\n",
    "\n",
    "  # When creating the reader, you must specify a database that is available within your Athena\n",
    "  # sandbox workgroup. Temporary tables associated with the sandbox files that are read directly\n",
    "  # as files, as well as those created when using the `df.cache()` method, will be created under this database.\n",
    "  # These temporary tables and temporary files will be deleted at the end when calling the reader’s `exit()` method.\n",
    "\n",
    "  spark = get_spark(\n",
    "        database_tmp=base_datos_temporal,\n",
    "        path_tmp=directorio_temporal,\n",
    "        workgroup=workgroup\n",
    "    )\n",
    "else:\n",
    "  spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CsxhxWgrQ7Fb"
   },
   "source": [
    "# 4. Data Reading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7seAA7s7Q_SE"
   },
   "source": [
    "## 4.1 Table Reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v1epKKLAQ-fN"
   },
   "outputs": [],
   "source": [
    "base_datos = \"__YOUR_DATABASE__\"\n",
    "table_1 =  \"__YOUR_TABLE_1__\"\n",
    "df = dataproc.read().table(base_datos + \".\" + table_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mwAgiIb3RF_J"
   },
   "source": [
    "## 4.2 Reading Parquet File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GT_5ychsRE6I"
   },
   "outputs": [],
   "source": [
    "df_parquet = dataproc.read().parquet('s3://__YOUR_S3_PATH_PARQUET__')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c-2OpyGtRKFX"
   },
   "source": [
    "## 4.3 Reading CSV File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YnXc1SuZQQeF"
   },
   "outputs": [],
   "source": [
    "# In Athena, CSV files are read through the directory that contains them — \n",
    "# you cannot specify the full path including the CSV file name.\n",
    "# The directory must contain only CSV files with the same structure.\n",
    "# If you want to read a single CSV file, its parent directory should contain only that file.\n",
    "\n",
    "ruta_lectura = 's3://__YOUR_S3_PATH_CSV__'\n",
    "df_csv = (\n",
    "    dataproc.read()\n",
    "    .option('inferSchema', 'false')\n",
    "    .option('header', 'true')\n",
    "    .option('sep', ';')\n",
    "    .option('encoding', 'latin1')  # ✅ forzar encoding\n",
    "    .csv(ruta_lectura)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7u62LYZsSCj5"
   },
   "source": [
    "# 5.Example of Transformations Using PySpark-like Code Executed by the Library Under Python + Athena"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2eYBjjCKSfwa"
   },
   "outputs": [],
   "source": [
    "base_datos = \"__YOUR_DATABASE__\"\n",
    "table_1 =  \"__YOUR_TABLE_1__\"\n",
    "table_2 = \"__YOUR_TABLE_2__\"\n",
    "table_3 = \"__YOUR_TABLE_3__\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SD_D4u8NSgw4"
   },
   "outputs": [],
   "source": [
    "def max_fecha(df):\n",
    "    fechas = df.select(F.max(F.col(\"closing_date\")).alias(\"closing_date\"))\n",
    "    fechas = fechas.withColumn(\"closing_date\",F.col(\"closing_date\").cast(T.StringType()))\n",
    "    maxFechaTabla = fechas.head().closing_date[0]\n",
    "    return maxFechaTabla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wedzBXGsR90y"
   },
   "outputs": [],
   "source": [
    "df = dataproc.read().table(base_datos + \".\" + table_1)\n",
    "print(\"Number of rows df:\" + str(df.count()))\n",
    "\n",
    "FECHA_PROCESO = max_fecha(df)\n",
    "\n",
    "print(FECHA_PROCESO)\n",
    "\n",
    "empleados = dataproc.read().table(base_datos + \".\" + table_2)\n",
    "# Filtro no necesario por ser tabla diaria\n",
    "# empleados = empleados.where(col(\"closing_date\")==f.to_date(FECHA_PROCESO))\n",
    "\n",
    "print(\"Number of rows in table_2:\" + str(empleados.count()))\n",
    "\n",
    "empleados=empleados.withColumn(\"employee_id\",F.trim(F.col(\"employee_id\")))\n",
    "empleados= empleados.select(\"employee_id\", \"user_id\", \"entity_id\", \"branch_id\").distinct()\n",
    "\n",
    "ofis = dataproc.read().table(base_datos + \".\" + table_3)\n",
    "ofis = ofis.where((F.col(\"entity_id\")==\"0182\") & (F.col(\"closing_date\") == max_fecha(ofis)))\n",
    "ofis=ofis.where( (F.col(\"level55_gen_manag_id\")==\"6055\") )\n",
    "\n",
    "print(\"Number of Rows in table_3:\" + str(ofis.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yU_nx7heR_EE"
   },
   "outputs": [],
   "source": [
    "empleados_join = ofis.join(empleados, on = ['entity_id', 'branch_id'] , how='inner')\n",
    "\n",
    "print(\"Number of Rows after Join:\" + str(empleados.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sFUq5VMUUUh4"
   },
   "outputs": [],
   "source": [
    "df = df.where(F.col(\"closing_date\")==F.to_date(F.lit(FECHA_PROCESO)))\n",
    "df=df.withColumn(\"matricula\",F.substring(F.col(\"g_worker_id\"),5,7) )\n",
    "df=df.withColumn(\"Estructurales\",F.substring(F.col(\"g_worker_id\"),4,1) )\n",
    "\n",
    "#Loes estrcuturales (ES y BB) tienen en la 4º posición el campo g_worker_id una \"S\"\n",
    "df=df.where(F.col(\"Estructurales\")==\"S\")\n",
    "\n",
    "empleados = empleados.withColumnRenamed('user_id', 'user_id_empleado')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NU6I4o1kl5_Y"
   },
   "outputs": [],
   "source": [
    "# Creamos esta columna solo para ejemplificar uso de Window\n",
    "empleados_ejemplo_window = empleados.withColumn('saldo', F.lit(1000))\n",
    "window = Window.partitionBy(\"employee_id\").orderBy(\"user_id_empleado\")   #\n",
    "\n",
    "empleados_ejemplo_window = empleados_ejemplo_window.withColumn(\"saldo_anterior\", F.lag(F.col(\"saldo\")).over(window))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_O89TAqVUdhD"
   },
   "outputs": [],
   "source": [
    "df_join = df.join(empleados, (df.matricula==empleados.employee_id), how = 'inner')\n",
    "\n",
    "print(\"Number of rows df:\" + str(df_join.count()))\n",
    "\n",
    "df_join = df_join.select(\"g_worker_id\",\"g_job_profile_id\",\"gf_wrk_posn_ocpn_start_date\",\"audit_date\",\"closing_date\",\"matricula\",\"Estructurales\",\"user_id\")\n",
    "\n",
    "print(\"Number of rows df:\" + str(df_join.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sD3FGe2yUsir"
   },
   "source": [
    "# 6. Writing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "13q3vAlUU4Q0"
   },
   "source": [
    "## 6.1 Writing in Parquet Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rhl0Dx8NVCb9"
   },
   "outputs": [],
   "source": [
    "dataproc.write().partitionBy(['closing_date']).option('partitionOverwriteMode', 'dynamic').mode(\n",
    "        'overwrite').parquet(ofis, 's3://__YOUR_S3_PATH_TO_SAVE_PARQUET__')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iLP7B1sJU4hY"
   },
   "source": [
    "## 6.2 Writing in CSV Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EizvUOKmfeHm"
   },
   "outputs": [],
   "source": [
    " #.option(\"header\", True) \\  No permitido guardar con cabeceras, unload guarda sin cabeceras\n",
    "dataproc.write().option(\"sep\", \";\").option('header', 'true').mode(\"overwrite\").partitionBy([\"closing_date\"]).csv(\n",
    "    ofis, \"s3://__YOUR_S3_PATH_TO_SAVE_CSV__\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "csFyvfihfjdn"
   },
   "source": [
    "# 7. Notebook Cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G8peMz3tfrcM"
   },
   "outputs": [],
   "source": [
    "if python:\n",
    "  datos_escaneados = spark._reader.data_scanned\n",
    "\n",
    "  print(f\"Coste: {(datos_escaneados/(1024*1024*1024*1024)) * 5.65} usd\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UCIcgn6eQa3h"
   },
   "source": [
    "# 8. Cleanup of Tables and Temporary Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LmFX2y1oQhTS"
   },
   "outputs": [],
   "source": [
    "if python:\n",
    "  # It is important to call the `stop()` method to remove temporary tables and files.\n",
    "  spark.stop()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
